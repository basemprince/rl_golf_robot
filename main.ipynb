{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f14ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from gymnasium.utils.save_video import save_video\n",
    "from sai_rl import SAIClient\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c65491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "TOTAL_TIMESTEPS = 1_000_000\n",
    "VIDEO_DIR = \"./ppo_videos/\"\n",
    "LOG_DIR = \"./ppo_logs/\"\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "CURRICULUM_STAGES = [\n",
    "    {\"difficulty\": 0.1, \"reward_bonus\": 0.0},\n",
    "    {\"difficulty\": 0.3, \"reward_bonus\": 0.2},\n",
    "    {\"difficulty\": 0.6, \"reward_bonus\": 0.4},\n",
    "    {\"difficulty\": 1.0, \"reward_bonus\": 0.6},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: BaseAlgorithm,\n",
    "    num_episodes: int = 10,\n",
    "    deterministic: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an RL agent for `num_episodes` using its VecEnv.\n",
    "\n",
    "    :param model: the RL Agent\n",
    "    :param num_episodes: number of episodes to evaluate it\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :return: Mean reward across episodes\n",
    "    \"\"\"\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "\n",
    "    all_episode_rewards = []\n",
    "    episode_rewards = []\n",
    "    episode_count = 0\n",
    "\n",
    "    while episode_count < num_episodes:\n",
    "        action, _states = model.predict(obs, deterministic=deterministic)\n",
    "        obs, rewards, dones, infos = vec_env.step(action)\n",
    "\n",
    "        episode_rewards.append(rewards[0])  # Only one env in DummyVecEnv\n",
    "\n",
    "        if dones[0]:\n",
    "            all_episode_rewards.append(sum(episode_rewards))\n",
    "            episode_rewards = []\n",
    "            episode_count += 1\n",
    "\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    std_reward = np.std(all_episode_rewards)\n",
    "    print(f\"✅ Evaluation complete: Mean reward = {mean_reward:.2f} ± {std_reward:.2f} over {num_episodes} episodes\")\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea628b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, stages, steps_per_stage=250_000):\n",
    "        super().__init__(env)\n",
    "        self.stages = stages\n",
    "        self.steps_per_stage = steps_per_stage\n",
    "        self.current_stage = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self._apply_difficulty()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward += self.stages[self.current_stage][\"reward_bonus\"]\n",
    "\n",
    "        self.total_steps += 1\n",
    "        self._update_stage()  # Check every step for smooth curriculum\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _update_stage(self):\n",
    "        success = self.env.get_attr(\"success_rate\")[0] if hasattr(self.env, \"get_attr\") else None\n",
    "        if success and success > 0.7 and self.current_stage < len(self.stages) - 1:\n",
    "            self.current_stage += 1\n",
    "            self._apply_difficulty()\n",
    "            print(f\"✅ Success {success:.2f}, advancing to stage {self.current_stage}\")\n",
    "        else:\n",
    "            # Fallback to step-based if success metric is not available\n",
    "            new_stage = min(self.total_steps // self.steps_per_stage, len(self.stages) - 1)\n",
    "            if new_stage != self.current_stage:\n",
    "                self.current_stage = new_stage\n",
    "                self._apply_difficulty()\n",
    "                print(\n",
    "                    f\"➡ Curriculum: Stage {self.current_stage} | Difficulty={self.stages[self.current_stage]['difficulty']}\"\n",
    "                )\n",
    "\n",
    "    def _apply_difficulty(self):\n",
    "        if hasattr(self.env, \"set_difficulty\"):\n",
    "            self.env.set_difficulty(self.stages[self.current_stage][\"difficulty\"])\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    sai = SAIClient(comp_id=\"franka-ml-hiring\")\n",
    "    env = sai.make_env(render_mode=\"rgb_array\")\n",
    "    env = Monitor(env)\n",
    "    return CurriculumWrapper(env, CURRICULUM_STAGES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "franka-golf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
